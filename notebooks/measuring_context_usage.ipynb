{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Context Usage with CXMI\n",
    "\n",
    "This notebook contains the code to measure CXMI for contextual models trained in this libray\n",
    "\n",
    "Start by setting the path for your checkpoint of interest. This should ideally be a model trained with *dynamic* context size. \n",
    "We also need set context size for which we are measuring CXMI. We also need to set the languages in order to load the sentencepiece models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt=\"/projects/tir5/users/patrick/checkpoints/iwslt2017/en-fr/one_to_five_1/\"\n",
    "source_context_size=0\n",
    "target_context_size=1\n",
    "source_lang=\"en\"\n",
    "target_lang=\"fr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then load the models and associated files such as the vocabularies into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece as sp\n",
    "import contextual_mt\n",
    "from fairseq import utils, hub_utils\n",
    "\n",
    "package = hub_utils.from_pretrained(\n",
    "    model_ckpt, checkpoint_file=\"checkpoint_best.pt\"\n",
    ")\n",
    "models = package[\"models\"]\n",
    "for model in models:\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "# load dict, params and generator from task\n",
    "src_dict = package[\"task\"].src_dict\n",
    "tgt_dict = package[\"task\"].tgt_dict\n",
    "\n",
    "# load sentencepiece models (assumes they are in the checkpoint dirs)\n",
    "# FIXME: is there someway to have it in `package`\n",
    "if os.path.exists(os.path.join(model_ckpt, \"spm.model\")):\n",
    "    spm = sp.SentencePieceProcessor()\n",
    "    spm.Load(os.path.join(model_ckpt, \"spm.model\"))\n",
    "    src_spm = spm\n",
    "    tgt_spm = spm\n",
    "else:\n",
    "    src_spm = sp.SentencePieceProcessor()\n",
    "    src_spm.Load(os.path.join(model_ckpt, f\"spm.{source_lang}.model\"))\n",
    "    tgt_spm = sp.SentencePieceProcessor()\n",
    "    tgt_spm.Load(os.path.join(model_ckpt, f\"spm.{target_lang}.model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring CXMI\n",
    "\n",
    "To measure CXMI, we need an held-out dataset. Currently, two types of dataset are supported\n",
    "\n",
    "* A standard dataset\n",
    "* A contrastive dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Dataset \n",
    "\n",
    "To measure the CXMI for standart dataset, define the source, target and docids files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_file=\"/projects/tir1/corpora/dialogue_mt/iwslt2017/en-fr/test.en-fr.en\"\n",
    "target_file=\"/projects/tir1/corpora/dialogue_mt/iwslt2017/en-fr/test.en-fr.fr\"\n",
    "docids_file=\"/projects/tir1/corpora/dialogue_mt/iwslt2017/en-fr/test.en-fr.docids\"\n",
    "batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the following cell to compute the corpus-level cxmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009629978\n"
     ]
    }
   ],
   "source": [
    "from contextual_mt.utils import parse_documents\n",
    "from contextual_mt.docmt_cxmi import compute_cxmi\n",
    "import numpy as np\n",
    "\n",
    "# load files needed\n",
    "with open(source_file, \"r\") as src_f:\n",
    "    srcs = [line.strip() for line in src_f]\n",
    "with open(docids_file, \"r\") as docids_f:\n",
    "    docids = [int(idx) for idx in docids_f]\n",
    "with open(target_file, \"r\") as tgt_f:\n",
    "    refs = [line.strip() for line in tgt_f]\n",
    "\n",
    "documents = parse_documents(srcs, refs, docids)\n",
    "sample_cxmis, ids = compute_cxmi(\n",
    "        documents,\n",
    "        models,\n",
    "        src_spm,\n",
    "        src_dict,\n",
    "        tgt_spm,\n",
    "        tgt_dict,\n",
    "        source_context_size,\n",
    "        target_context_size,\n",
    "        batch_size=batch_size\n",
    ")\n",
    "print(np.mean(sample_cxmis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive dataset\n",
    "\n",
    "To compute CXMI for either ContraPro or Bawden's contrastive dataset, start by defining the dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022161597\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from contextual_mt.docmt_contrastive_eval import load_contrastive\n",
    "from contextual_mt.contextual_dataset import collate\n",
    "from contextual_mt.utils import encode, decode, create_context\n",
    "from fairseq.sequence_scorer import SequenceScorer\n",
    "\n",
    "bawden=True\n",
    "source_file=\"/home/pfernand/repos/discourse-mt-test-sets/test-sets/lexical_choice.current.en\"\n",
    "target_file=\"/home/pfernand/repos/discourse-mt-test-sets/test-sets/lexical_choice.current.fr\"\n",
    "src_context_file=\"/home/pfernand/repos/discourse-mt-test-sets/test-sets/lexical_choice.prev.en\"\n",
    "tgt_context_file=\"/home/pfernand/repos/discourse-mt-test-sets/test-sets/lexical_choice.prev.fr\"\n",
    "#source_file=\"/home/pfernand/repos/ContraPro/contrapro.text.en\"\n",
    "#target_file=\"/home/pfernand/repos/ContraPro/contrapro.text.de\"\n",
    "#src_context_file=\"/home/pfernand/repos/ContraPro/contrapro.context.en\"\n",
    "#tgt_context_file=\"/home/pfernand/repos/ContraPro/contrapro.context.de\"\n",
    "\n",
    "\n",
    "# load files\n",
    "srcs, all_tgts, tgt_labels, srcs_contexts, tgts_contexts = load_contrastive(\n",
    "    source_file, target_file, src_context_file, tgt_context_file, dataset=\"bawden\" if bawden else \"contrapro\"\n",
    ")\n",
    "\n",
    "scorer = SequenceScorer(tgt_dict)\n",
    "sample_cxmis = []\n",
    "corrects = []\n",
    "b_corrects = []\n",
    "for src, src_ctx, contr_tgts, tgt_ctx in zip(srcs, srcs_contexts, all_tgts, tgts_contexts):\n",
    "    src = encode(src, src_spm, src_dict)\n",
    "    src_ctx = [encode(ctx, src_spm, src_dict) for ctx in src_ctx]\n",
    "    contr_tgts = [encode(tgt, tgt_spm, tgt_dict) for tgt in contr_tgts]\n",
    "    tgt_ctx = [encode(ctx, tgt_spm, tgt_dict) for ctx in tgt_ctx]\n",
    "    baseline_samples = []\n",
    "    contextual_samples = []\n",
    "\n",
    "    for tgt in contr_tgts:\n",
    "        baseline_src_context = create_context(\n",
    "            src_ctx,\n",
    "            0,\n",
    "            break_id=src_dict.index(\"<brk>\"),\n",
    "            eos_id=src_dict.eos(),\n",
    "        )\n",
    "        baseline_tgt_context = create_context(\n",
    "            tgt_ctx,\n",
    "            0,\n",
    "            break_id=tgt_dict.index(\"<brk>\"),\n",
    "            eos_id=tgt_dict.eos(),\n",
    "        )\n",
    "        contextual_src_context = create_context(\n",
    "            src_ctx,\n",
    "            source_context_size,\n",
    "            break_id=src_dict.index(\"<brk>\"),\n",
    "            eos_id=src_dict.eos(),\n",
    "        )\n",
    "        contextual_tgt_context = create_context(\n",
    "            tgt_ctx,\n",
    "            target_context_size,\n",
    "            break_id=tgt_dict.index(\"<brk>\"),\n",
    "            eos_id=tgt_dict.eos())\n",
    "\n",
    "        full_src = torch.cat([src, torch.tensor([src_dict.eos()])])\n",
    "        full_tgt = torch.cat([tgt, torch.tensor([tgt_dict.eos()])])\n",
    "        baseline_sample = {\n",
    "            \"id\": 0,\n",
    "            \"source\": full_src,\n",
    "            \"src_context\": baseline_src_context,\n",
    "            \"target\": full_tgt,\n",
    "            \"tgt_context\": baseline_tgt_context,\n",
    "        }\n",
    "        contextual_sample = {\n",
    "            \"id\": 0,\n",
    "            \"source\": full_src,\n",
    "            \"src_context\": contextual_src_context,\n",
    "            \"target\": full_tgt,\n",
    "            \"tgt_context\": contextual_tgt_context,\n",
    "        }\n",
    "        baseline_samples.append(baseline_sample)\n",
    "        contextual_samples.append(contextual_sample)\n",
    "\n",
    "    baseline_sample = collate(\n",
    "        baseline_samples,\n",
    "        pad_id=src_dict.pad(),\n",
    "        eos_id=src_dict.eos(),\n",
    "    )\n",
    "    contextual_sample = collate(\n",
    "        contextual_samples,\n",
    "        pad_id=src_dict.pad(),\n",
    "        eos_id=src_dict.eos()\n",
    "    )\n",
    "\n",
    "    baseline_sample = utils.move_to_cuda(baseline_sample)\n",
    "    contextual_sample = utils.move_to_cuda(contextual_sample)\n",
    "\n",
    "    baseline_out = scorer.generate(models, baseline_sample)\n",
    "    contextual_out = scorer.generate(models, contextual_sample)\n",
    "\n",
    "    scores = [h[0][\"score\"] for h in contextual_out]\n",
    "\n",
    "    most_likely = torch.argmax(torch.stack(scores))\n",
    "    correct = most_likely == 0\n",
    "    baseline_correct = torch.argmax(torch.stack([h[0][\"score\"] for h in baseline_out])) == 0\n",
    "\n",
    "    b_corrects.append(baseline_correct)\n",
    "    corrects.append(correct)\n",
    "    sample_cxmis.append(contextual_out[0][0][\"score\"].cpu() - baseline_out[0][0][\"score\"].cpu())\n",
    "\n",
    "corrects = np.stack([correct.cpu().numpy() for correct in corrects])\n",
    "b_corrects = np.stack([b_correct.cpu().numpy() for b_correct in b_corrects])\n",
    "print(np.mean(sample_cxmis))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring Correlations\n",
    "\n",
    "To measure the correlation of the *per-sample* CXMI with the performance on samples that requires context, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointbiserialrResult(correlation=0.025460697287605066, pvalue=0.7204408014491894)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "binary_vars = np.stack([not b_c and c for b_c, c in zip(b_corrects, corrects)])\n",
    "print(scipy.stats.pointbiserialr(binary_vars, sample_cxmis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Samples\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current: ('Genius.', 'Génial.')\n",
      "context: (\"I am learning that it's a genius idea to use a pair of barbecue tongs  to pick up things that you dropped.   I'm learning that nifty trick where you can charge  your mobile phone battery from your chair battery.\", \"J'apprends que c'est une idée géniale d'utiliser une pince à barbecue pour ramasser les choses qu'on a laissé tomber.  J'apprends ce truc génial pour charger la batterie de son téléphone portable grâce à celle de son fauteuil.\")\n",
      "\n",
      "current: ('This is the thing about symbols.', 'avec les symboles :')\n",
      "context: ('This is the thing about postmodernism.', \"C'est le problème avec le post-modernisme,\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dbc833a67697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"current: {documents[i[0]][i[1]]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"context: {documents[i[0]][i[1]-1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/dialog_mt/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dialog_mt/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _, i in sorted(zip(sample_cxmis, ids), reverse=True):\n",
    "    print(f\"current: {documents[i[0]][i[1]]}\")\n",
    "    print(f\"context: {documents[i[0]][i[1]-1]}\")\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('dialog_mt': conda)",
   "language": "python",
   "name": "python37964bitdialogmtcondaaf9d4fe34c0b48b29c1c56956f51d584"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
